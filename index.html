<!DOCTYPE html>
<html lang="en">
<head>
<title>tiger</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="css/home.css" media="screen"/>
</head>
<body>

<div class="header">
  <h1>The Information retrieval GEneral Reading (TIGER) Group</h1>
    <h2>RMIT University</h2>
</div>

<div class="row">
  <div class="side">
    <p>Andrew Turpin founded TIGER in 2006 and organized weekly meetings for quite some time.</p>
      <!--<img src="img/tiger.jpg" alt="Tiger Image" class="tigerimg">-->
      <p>Students, staff, and interested parties outside the university are welcome to attend.</p>
      <p>We encourage all members to suggest new research topics, guide, and participate in the weekly discussions.</p>
      <p>For inquiries about joining TIGER or guiding a discussion, please contact Elisa Mena <"firstname dot lastname at student dot rmit dot edu dot au">.</p>

  </div>

  <div class="main">
    <h2>Talks 2021</h2>
    <h3>The talks are held every Thursday from 12:30 to 13:30 (GMT+10, Melbourne Australia Time) unless other times specified</h3>

  <table>
    <colgroup>
      <col class="date">
      <col class="speaker">
      <col class="title">
      <col class="abstract">
    </colgroup>
    <caption>September</caption>
      <tr>
        <th>Date</th>
        <th>Speaker</th>
        <th>Title</th>
        <th>Abstract</th>
      </tr>
      <tr>
        <td>02/09/2021</td>
        <td>RMIT</td>
        <td>SLOW DOWN</td>
        <td>WEEK</td>
      </tr>
      <tr>
        <td>08/07/2021</td>
        <td>Xi Wang<div><a href="https://web.microsoftstream.com/video/0e71d437-818e-4fbc-8517-3088dc85bb0b">Recording</a></div></td>
        <td>Personalised Usefulness of Reviews for Effective Recommendation</td>
        <td>Recent review-based recommenders have shown promising results in improving the recommendation performance on various public datasets. However, to make effective recommendations, it is both vital and challenging to accurately measure the usefulness of reviews. In particular, according to the literature, users have been shown to exhibit distinct preferences over different types of reviews (e.g. preferring longer vs. shorter or recent vs. old reviews). Yet, there have been limited studies that account for the personalised usefulness of reviews when estimating the users' preferences. This talk presents two consecutive studies in addressing such a research gap in the literature: 1) NCWS, a weakly supervised binary review helpfulness classifier; 2) RPRS, an end-to-end recommendation model that estimates users preferences first over the reviews exhibiting various properties and then over the items of interest.</td>
      </tr>
      <tr>
        <td>09/09/2021</td>
        <td>Chen Zhao<div><a href="https://rmiteduau-my.sharepoint.com/:v:/g/personal/s3736298_student_rmit_edu_au/Eff6NxDiEaBCggJwV-TFihQBb0xDHHzHLnNLsiu5H6k7fA?email=s3736298%40student.rmit.edu.au">Recording</a></div></td>
        <td>Towards more practical complex question answering</td>
        <td>Question answering is one of the most important and challenging tasks for understanding human language. With the help of large-scale benchmarks, state-of-the-art neural methods have made significant progress to even answer complex questions that require multiple evidence pieces. Nevertheless, training existing SOTA models requires several assumptions (e.g., intermediate evidence annotation,  corpus semi-structure) that limit the applicability to only academic testbeds. In this talk, I discuss several solutions to make current QA systems more practical.  I first describe a state-of-the-art system for complex QA with an extra hop attention in its layers to aggregate different pieces of evidence following the structure. Then I introduce a dense retrieval approach that iteratively forms an evidence chain through beam search in dense representations, without using semi-structured information. Finally, I describe a dense retrieval work that focuses on a weakly-supervised setting, by learning to find evidence from a large corpus, and relying only on distant supervision for model training. </td>
      </tr>
      <tr>
        <td>16/09/2021</td>
        <td>Sachin Pathiyan Cherumanal</td>
        <td>Evaluating Fairness in Argument Retrieval<div><a href="https://arxiv.org/pdf/2108.10442.pdf">Arxiv Paper</a></div></td>
        <td>Existing commercial search engines often struggle to represent different perspectives of a search query. Argument retrieval systems address this limitation of search engines and provide both positive (PRO) and negative (CON) perspectives about a user’s information need on a controversial topic (e.g., climate change). The effective-ness of such argument retrieval systems is typically evaluated based on topical relevance and argument quality, without taking into account the often differing number of documents shown for the argument stances (PRO or CON). Therefore, systems may retrieve relevant passages, but with a biased exposure of arguments. In this work, we analyze a range of non-stochastic fairness-aware ranking and diversity metrics to evaluate the extent to which argument stances are fairly exposed in argument retrieval systems. Using the official runs of the argument retrieval task Touché at CLEF 2020, as well as synthetic data to control the amount and order of argument stances in the rankings, we show that systems with the best effectiveness in terms of topical relevance are not necessarily the most fair or the most diverse in terms of argument stance. The relationships we found between (un)fairness and diversity metrics shed light on how to evaluate group fairness – in addition to topical relevance – in argument retrieval settings.</td>
      </tr>
      <tr>
        <td>23/09/2021</td>
        <td>Laurianne Sitbon</td>
        <td>Towards Inclusive Interactions in Information Retrieval</td>
        <td>The World Wide Web of 2021 is a platform for sharing, socialising and synthesizing vast amounts of information. However, for the approximately 3% of the population with intellectual disability, access remains limited. Most people with intellectual disability (ID) have reduced abilities to digest new or complex information, requiring specific accessible design. Yet they often do not fit a neatly labeled diagnostic category, often having a combination of underlying cognitive, communicative, motor and sensory conditions. In this talk, Laurianne will present what she and her team learnt through 5 years of fieldwork, co-designing interactive information access systems with adults with intellectual disability. She will demonstrate with examples how iterative approaches that centre on people’s competencies, and recognise support networks as part of key competencies, can ensure future designs are both inclusive and respectful of individuals with intellectual disability. She will discuss opportunities for continuing research to discover how recommender systems can better support visual and multimodal interactions in people’s own terms.</td>
      </tr>
      <tr>
        <td>30/09/2021</td>
        <td>Dana Mcay</td>
        <td></td>
        <td></td>
      </tr>
</table>
  </br>
  <table>
    <colgroup>
      <col class="date">
      <col class="speaker">
      <col class="title">
      <col class="abstract">
    </colgroup>
    <caption>August</caption>
      <tr>
        <th>Date</th>
        <th>Speaker</th>
        <th>Title</th>
        <th>Abstract</th>
      </tr>
      <tr>
        <td>05/08/2021</td>
        <td>Manuel Steiner</td>
        <td>Untangling the Concept of Task in Information Seeking and Retrieval, ICTIR 2021 (paper reading)</td>
        <td>The paper discusses tasks in Information Seeking and Retrieval. Researchers of past works often use different terminology to describe a concept or some terms to refer to different concepts when discussing tasks in ISR. The authors of this paper first provide an overview of previous literature. They highlight commonalities and differences between task hierarchies and surrounding concepts. They note that work roles are commonly absent from previous works. The paper then presents an integrated task taxonomy based on existing literature, consolidating concepts into a single model, which also includes work roles, based on the review of literature in work structure, management, and human resources.</td>
      </tr>
      <tr>
        <td>12/08/2021</td>
        <td>-</td>
        <td>--</td>
        <td>-</td>
      </tr>
      <tr>
        <td>19/08/2021</td>
        <td>Vincent Li<div><a href="https://web.microsoftstream.com/video/a8e7929c-3083-4402-887a-8a93eb8f5862">Recording</a></div></td>
        <td>Challenges and Learnings in Product Search</td>
        <td>Product search is a specific type of search engine adopted by online businesses to help customers find relevant products. E-commerce search, job search, or hotel search are different applications of product search. Product search problems are different from conventional web search engines in many ways. For example, in product search, the documents are usually less descriptive and more structured, which makes classical retrieval models less effective. The search intents for product search engines are usually more than finding relevant documents and the definition of relevance has multiple dimensions rather than just topical relevance. Moreover, recall and precision are both important for product search, while the ranking of the products is critical for driving the business outcomes. All these differences pose new challenges for solving the product search problems. In this talk, I will discuss the existing challenges of product search problems and share some learnings working with product search applications.</td>
      </tr>
      <tr>
        <td>26/08/2021</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
</table>
  </br>
  <table>
    <colgroup>
      <col class="date">
      <col class="speaker">
      <col class="title">
      <col class="abstract">
    </colgroup>
    <caption>July</caption>
      <tr>
        <th>Date</th>
        <th>Speaker</th>
        <th>Title</th>
        <th>Abstract</th>
      </tr>
      <tr>
        <td>01/07/2021</td>
        <td>Danula Hettiachchi</td>
        <td>The Challenge of Variable Effort Crowdsourcing <div><a href="https://arxiv.org/abs/2105.09457">Paper</a></div></td>
        <td>We consider a class of variable effort human annotation tasks in which the number of labels required per item can greatly vary (e.g., finding all faces in an image, named entities in a text, bird calls in an audio recording, etc.). In such tasks, some items require far more effort than others to annotate. Furthermore, the per-item annotation effort is not known until after each item is annotated since determining the number of labels required is an implicit part of the annotation task itself. On an image bounding-box task with crowdsourced annotators, we show that annotator accuracy and recall consistently drop as effort increases. We hypothesize reasons for this drop and investigate a set of approaches to counteract it. Firstly, we benchmark on this task a set of general best-practice methods for quality crowdsourcing. Notably, only one of these methods actually improves quality: the use of visible gold questions that provide periodic feedback to workers on their accuracy as they work. Given these promising results, we then investigate and evaluate variants of the visible gold approach, yielding further improvement. Final results show a 7% improvement in bounding-box accuracy over the baseline. We discuss the generality of the visible gold approach and promising directions for future research.</td>
      </tr>
      <tr>
        <td>08/07/2021</td>
        <td>Xi Wang</td>
        <td>Personalised Usefulness of Reviews for Effective Recommendation</td>
        <td>Recent review-based recommenders have shown promising results in improving the recommendation performance on various public datasets. However, to make effective recommendations, it is both vital and challenging to accurately measure the usefulness of reviews. In particular, according to the literature, users have been shown to exhibit distinct preferences over different types of reviews (e.g. preferring longer vs. shorter or recent vs. old reviews). Yet, there have been limited studies that account for the personalised usefulness of reviews when estimating the users' preferences. This talk presents two consecutive studies in addressing such a research gap in the literature: 1) NCWS, a weakly supervised binary review helpfulness classifier; 2) RPRS, an end-to-end recommendation model that estimates users preferences first over the reviews exhibiting various properties and then over the items of interest.</td>
      </tr>
      <tr>
        <td>15/07/2021</td>
        <td>SIGIR</td>
        <td>Conference</td>
        <td></td>
      </tr>
      <tr>
        <td>22/07/2021</td>
        <td></td>
        <td></td>
        <td></td>
      </tr>
      <tr>
        <td>29/07/2021</td>
        <td></td>
        <td></td>
        <td></td>
      </tr>
</table>
  </br>
  <table>
    <colgroup>
      <col class="date">
      <col class="speaker">
      <col class="title">
      <col class="abstract">
    </colgroup>
    <caption>June</caption>
      <tr>
        <th>Date</th>
        <th>Speaker</th>
        <th>Title</th>
        <th>Abstract</th>
      </tr>
  <tr>
    <td>03/06/2021</td>
    <td>Pablo Castells <div><a href="https://web.microsoftstream.com/video/5821d25a-c0be-438b-a0fe-55ff34575c4d">Recording</a></div></td>
    <td>Rational and irrational bias in recommendation <b>(17:00 GMT+10)</b></td>
    <td>Concern for bias in IR has grown considerably in the last few years, and recommender systems are a particular area where applications and experiments are immersed in bias. I will discuss some general angles on the biases that pervade recommendation, and what researchers are doing about it. I will follow up into some of my particular experience and findings in better understanding the effect of popularity biases on the effectiveness of recommendation and our capability to properly measure it in offline evaluation. I will discuss how the positive or negative effect of popularity in recommendation relates to the role of relevance in users' discoveries and choices, and in the formation of majorities, sometimes in non-obvious ways.</td>
  </tr>
  <tr>
    <td>10/06/2021</td>
    <td>Supervisors meeting</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>17/06/2021</td>
    <td>Nick Craswell<div><a href="https://web.microsoftstream.com/video/c815915e-0516-4068-977f-8271a5fd07f1">Recording</a></div></td>
    <td>Comparing and deploying deep learning models for Web Search</td>
    <td>In recent years, our idea of what rankers work best in Web search has completely changed. We used to believe that boosted tree models with hand-crafted features were the best approach, but new approaches, particularly with the current BERT-style rankers, seem significantly better. How do we convince ourselves that these models are really better? What’s the process we can use in a product to deploy improvements to ML rankers? To measure them in production? How do we deal with competing goals: Clicks, positive relevance labels, higher DAU/retention? I will describe some of our public-facing research in the area, since I can talk about that freely, but also link it to how things work in development and measurement of ML models in Bing (without revealing product details).</td>
  </tr>
  <tr>
    <td>24/06/2021</td>
    <td>The School of Engineering and Computing Technologies</td>
    <td>ECT Milestone Conference</td>
    <td></td>
  </tr>

    </table>
  </br>
    <table>
    <colgroup>
      <col class="date">
      <col class="speaker">
      <col class="title">
      <col class="abstract">
  </colgroup>
  <caption>May</caption>
  <tr>
    <th>Date</th>
    <th>Speaker</th>
    <th>Title</th>
    <th>Abstract</th>
  </tr>
  <tr>
    <td>06/05/2021</td>
    <td>Yongxin Xu</td>
    <td> Internet searching and stock price crash risk: Evidence from a quasi-natural experiment</td>
    <td>In 2010, Google unexpectedly withdrew its searching business from China, reducing investors’ ability to find information online. The stock price crash risk for firms searched for more via Google before its withdrawal subsequently increases by 19%, suggesting that Internet searching facilitates investors’ information processing. The sensitivity of stock returns to negative Internet posts also rises by 36%. The increase in crash risk is more pronounced when firms are more likely to hide adverse information and when information intermediaries are less effective in assisting investors’ information processing. In addition, liquidity (price delay) decreases (increases) after Google's withdrawal.</td>
  </tr>
  <tr>
    <td>13/05/2021</td>
    <td>Oleg Zendel</td>
    <td>An Enhanced Evaluation Framework for Query Performance Prediction</td>
    <td>Query Performance Prediction (QPP) has been studied extensively in the IR community over the last two decades. A by-product of this research is a methodology to evaluate the effectiveness of QPP techniques. In this paper, we re-examine the existing evaluation methodology commonly used for QPP, and propose a new approach. Our key idea is to model QPP performance as a distribution instead of relying on point estimates. Our work demonstrates important statistical implications, and overcomes key limitations imposed by the currently used correlation-based point-estimate evaluation approaches. We also explore the potential benefits of using multiple query formulations and ANalysis Of VAriance (ANOVA) modelling in order to measure interactions between multiple factors. The resulting statistical analysis combined with a novel evaluation framework demonstrates the merits of modelling QPP performance as distributions, and enables detailed statistical ANOVA models for comparative analyses to be created.</td>
  </tr>
  <tr>
    <td>20/04/2021</td>
    <td>Paul Thomas</td>
    <td>Do affective cues validate behavioural metrics for search?</td>
    <td>Traces of searcher behaviour, such as query reformulation or clicks, are commonly used to evaluate a running search engine. The underlying expectation is that these behaviours are proxies for something more important, such as relevance, utility, or satisfaction. Affective computing technology gives us the tools to help confirm some of these expectations, by examining visceral expressive responses during search sessions. However, work to date has only studied small populations in laboratory settings and with a limited number of contrived search tasks.
In this study, we analysed longitudinal, in-situ, search behaviours of 152 information workers, over the course of several weeks while simultaneously tracking their facial expressions. Results from over 20,000 search sessions and 45,000 queries allow us to observe that indeed affective expressions are consistent with, and complementary to, existing “click-based” metrics. On a query level, searches that result in a short dwell time are associated with a decrease in smiles (expressions of “happiness”) and that if a query is reformulated the results of the reformulation are associated with an increase in smiling – suggesting a positive outcome as people converge on the information they need. On a session level, sessions that feature reformulations are more commonly associated with fewer smiles and more furrowed brows (expressions of “anger/frustration”). Similarly, sessions with short-dwell clicks are also associated with fewer smiles. These data provide an insight into visceral aspects of search experience and present a new dimension for evaluating engine performance.
(This is work with Daniel McDuff, Nick Craswell, Kael Rowan, and Mary Czerwinski, all at Microsoft.)</td>
  </tr>
  <tr>
    <td>27/05/2021</td>
    <td>Romy Menghao Jia</td>
    <td>LGBTQ+ individuals’ information seeking and sharing in an online community</td>
    <td>Individuals with LGBTQ+ identities are at risk of negative outcomes due to their highly stigmatized sexual and gender minority status. Online communities are especially important to this population group, as they provide a safe and validating space for building interpersonal connections and fostering a sense of belonging. Focused on the affective and motivational aspects of LGBTQ+ individuals’ information behaviour, our work investigates the emotions LGBTQ+ individuals expressed, and their relatedness needs that motivated them to seek and share information in an online community. Through a deductive thematic analysis of 156 posts to an LGBTQ+ online forum, our analysis reveals three main categories of relatedness needs: being cared about, caring for others, as well as building and maintaining relationships. Sixty-one posts that contained emotional texts were further coded to analyse the emotions expressed by LGBTQ+ individuals. Seven categories of emotions emerged from the analysis: fear, uncertainty, sadness, anger, shame, joy, and others. Our work contributes to the existing knowledge of how LGBTQ+ individuals cope with various challenges and how online communities can better support sexual and gender minority people. Future work will investigate the influence of their information behaviour on their resilience and affordances implications for LGBTQ+ online community design. </td>
  </tr>

</table>
  </br>
    <table>
    <colgroup>
      <col class="date">
      <col class="speaker">
      <col class="title">
      <col class="abstract">
  </colgroup>
  <caption>April</caption>
  <tr>
    <th>Date</th>
    <th>Speaker</th>
    <th>Title</th>
    <th>Abstract</th>
  </tr>
  <tr>
    <td>01/04/2021</td>
    <td>Leila Tavakoli</td>
    <td>Analysing Clarification in Asynchronous Information Seeking Conversations</td>
    <td>This research analyses human-generated clarification questions to provide insights into how they are used to disambiguate and provide a better understanding of information needs. A set of clarification questions is extracted from posts on the Stack Exchange platform. Novel taxonomy is defined for the annotation of the questions and their responses. Our results indicate that questions answered by the person who submitted the original post (the asker) are more likely to be informative and to increase the chance of getting an accepted answer. After identifying which clarification questions are more useful, we investigate the characteristics of these questions in terms of their types and patterns. Non-useful clarification questions are identified, and their patterns are compared with useful clarifications. Our analysis indicates that the most useful clarification questions have similar patterns, regardless of topic. This research contributes to an understanding of clarification in conversations and can provide insight for clarification dialogues in conversational search scenarios and for the possible system generation of clarification requests in information seeking conversations.</td>
  </tr>
  <tr>
    <td>08/04/2021</td>
    <td>Valeriia Baranova</td>
    <td>A taxonomy of non-factoid questions</td>
    <td>Non-factoid question answering (NFQA) is a challenging task where a system should return complex long-form answers (such as explanations or opinions) to open-ended questions. Although researchers are working on datasets and systems in this area, so far the performance of the latter falls far behind systems created to answer factoid questions. The reason could be that the form of answers that an ideal NFQA system should return varies greatly depending on the category of asked question.
In our study, we propose the first comprehensive non-factoid question taxonomy constructed by employing grounded theory and extensively evaluated via several crowdsourcing studies. Along with taxonomy, we provide a dataset of non-factoid question categories and a performant model for question category prediction. Both will be made publicly available to the research community. Finally, we conducted an analysis of non-factoid question category distribution in various existing QA and conversational datasets, showing that the most challenging question categories for the existing NFQA systems are poorly represented in these datasets.
We believe that a better understanding of question categories of non-factoid questions and the structure of target answers will greatly aid the research in this area.</td>
  </tr>
  <tr>
    <td>15/04/2021</td>
    <td>Shohreh Deldari</td>
    <td>Time Series Change Point Detection with Self-Supervised Contrastive Predictive Coding</td>
    <td>Change Point Detection (CPD) methods identify the times associated with changes in the trends and properties of time series data in order to describe the underlying behaviour of the system. For instance, detecting the changes and anomalies associated with web service usage, application usage or human behaviour can provide valuable insights for downstream modelling tasks. We propose a novel approach for self-supervised Time Series Change Point detection method based on Contrastive Predictive coding (TS-CP^2). TS-CP^2 is the first approach to employ a contrastive learning strategy for CPD by learning an embedded representation that separates pairs of embeddings of time adjacent intervals from pairs of interval embeddings separated across time. Through extensive experiments on three diverse, widely used time series datasets, we demonstrate that our method outperforms five state-of-the-art CPD methods, which include unsupervised and semi-supervised approaches. TS-CP^2 is shown to improve the performance of methods that use either handcrafted statistical or temporal features by 79.4% and deep learning-based methods by 17.0% with respect to the F1-score averaged across the three datasets.</td>
  </tr>
  <tr>
    <td>22/04/2021</td>
    <td>Mark Sanderson<div><a href="https://web.microsoftstream.com/video/293f6e33-0030-438a-9fdb-721212f497ba">Recording</a></div></td>
    <td>How Do You Test a Test? Examining Significance from Different Angles</td>
    <td>In this talk, I will describe a suite of measures, which are jointly used to investigate a recently proposed ANOVA model based significance test and to compare it to two well-known baselines. I will apply the measures to the both the runs of a TREC track, and to the runs submitted by single participants. The former reveals test behaviour in the heterogeneous settings of a large-scale evaluation initiative, the latter lets us know what happens in the much more restricted case of variants of a single system. The results of our study show the novel ANOVA model to be substantially better than a commonly used significance test found in many IR researcher papers.</td>
  </tr>
  <tr>
    <td>29/04/2021</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>

</table>
  </br>
    <table>
        <colgroup>
          <col class="date">
          <col class="speaker">
          <col class="title">
          <col class="abstract">
        </colgroup>
        <caption>March</caption>
      <tr>
        <th>Date</th>
        <th>Speaker</th>
        <th>Title</th>
        <th>Abstract</th>
      </tr>
      <tr>
        <td>03/03/2021</td>
        <td>Rosie Jones</td>
        <td>Research on Podcasts at Spotify</td>
        <td>Podcasts are a large and growing repository of spoken audio. As an audio format, podcasts are more varied in style and production type than broadcast news, contain more genres than typically studied in video data, and are more varied in style and format than previous corpora of conversations. When transcribed with automatic speech recognition they represent a noisy but fascinating collection of documents which can be studied through the lens of natural language processing, information retrieval, and linguistics. Paired with the audio files, they are also a resource for speech processing and the study of acoustic aspects of the domain. We introduce the Spotify Podcast Dataset, a new corpus of 100,000 podcasts. We demonstrate the complexity of the domain with a case study of two tasks drawn from information retrieval and NLP: (1) passage search and (2) summarization. This data is orders of magnitude larger than previous speech corpora used for search and summarization. Our results show that the size and variability of this corpus opens up new avenues for research. I’ll discuss some of those avenues, as well as giving a brief overview of research on podcasts and music at Spotify.</td>
      </tr>
      <tr>
        <td>11/03/2021</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
      <tr>
        <td>18/03/2021</td>
        <td>CHIIR</td>
        <td>Conference</td>
        <td>-</td>
      </tr>
      <tr>
        <td>25/03/2021</td>
        <td>Yuta Saito</td>
        <td>Towards Realistic and Reproducible Off-Policy Evaluation: Open-Source Dataset, Software, and Application in Fashion E-Commerce Recommendation</td>
        <td>There is a growing interest in off-policy evaluation (OPE) or offline evaluation in web search and data mining; we have witnessed great research progress over the past decade. There is, however, a critical issue in the current OPE research; all existing experiments are either unrealistic or irreproducible, creating the gap between theory and practice. To break this gap and push forward OPE to a tangible method, we are running an open-source research project called the Open Bandit Project. The project includes Open Bandit Dataset and Open Bandit Pipeline. The Open Bandit Dataset is a real-world public dataset collected on the ZOZOTOWN platform, the largest fashion e-commerce in Japan. The dataset is unique; it contains two sets of log data collected by running multiple different recommendation policies, enabling fair comparisons of different OPE methods for the first time. We also implement a Python software, Open Bandit Pipeline, to streamline and standardize the implementation of OPE both in research and practice. In this talk, I will share the basic formulation and discuss current issues in OPE research. Then, I will introduce our open-source project and show a proof of concept live demonstration about OPE with our software. Finally, I will talk about some real-world applications in the ZOZOTOWN recommendation interface.</td>
      </tr>
    </table>
  </br>
  <h4>January and February was a restructuring period in 2021</h4>
  </div>
</div>
<div class="footer">
  <h4>Last updated: 03/06/2021</h4>
</div>
</body>
</html>
